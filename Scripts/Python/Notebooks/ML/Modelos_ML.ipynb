{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sxmuu/TG-Samuel-P/blob/main/Scripts/Python/Notebooks/ML/Modelos_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Ingeniería de Variables**"
      ],
      "metadata": {
        "id": "9CHS9X3FCmpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importaciones ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "# Utilidad: reindexar por día dentro de cada estación para ventanas móviles estrictas\n",
        "def reindex_daily_per_station(df, station_col='Estacion', date_col='Date'):\n",
        "    out = []\n",
        "    for est, dfg in df.groupby(station_col, sort=False):\n",
        "        dfg = dfg.sort_values(date_col).copy()\n",
        "        idx = pd.date_range(dfg[date_col].min(), dfg[date_col].max(), freq='D')\n",
        "        dfg = dfg.set_index(date_col).reindex(idx).rename_axis(date_col).reset_index()\n",
        "        dfg[station_col] = est\n",
        "        out.append(dfg)\n",
        "    return pd.concat(out, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "XMeA65Mh4ylD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Sxmuu/TG-Samuel-P/main/Databases/Contam/Final/df_final.xlsx\"\n",
        "\n",
        "df = pd.read_excel(url, engine=\"openpyxl\")  # instala openpyxl si hace falta"
      ],
      "metadata": {
        "id": "UYnmHHh0YnAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_cols = ['Date','Estacion','Localidad','PM25','lat','lon','Altitud',\n",
        "                 'Pres','Precip','Hum','Temp','WindSpeed']\n",
        "missing = [c for c in expected_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Faltan columnas esperadas: {missing}\")\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df = df.sort_values(['Estacion','Date']).drop_duplicates(subset=['Estacion','Date']).reset_index(drop=True)\n",
        "\n",
        "print(df[['Estacion','Date']].groupby('Estacion').agg(['min','max','nunique']).head())\n"
      ],
      "metadata": {
        "id": "ScKIy6RF7Flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2) (Opcional) Reindexar a diario por estación ---\n",
        "# Si ya sabes que todas las estaciones tienen una observación por día, puedes saltarte esto.\n",
        "# Si no, esto asegura ventanas móviles de longitud exacta (introducirá NaN si faltaban días).\n",
        "df = reindex_daily_per_station(df, station_col='Estacion', date_col='Date')\n"
      ],
      "metadata": {
        "id": "DOqD8Wzw7LSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2) Tipos y orden temporal ---\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "# (Opcional) Orden global\n",
        "df = df.sort_values(['Estacion', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# (Opcional) Quitar duplicados exactos por Estacion-Fecha (si existieran)\n",
        "df = df.drop_duplicates(subset=['Estacion','Date'])\n",
        "print(df[['Estacion','Date']].groupby('Estacion').agg(['min','max','nunique']).head())"
      ],
      "metadata": {
        "id": "ikyFe3kk48dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3) Calendario y estacionalidad ---\n",
        "df['year'] = df['Date'].dt.year\n",
        "df['month'] = df['Date'].dt.month\n",
        "df['dayofyear'] = df['Date'].dt.dayofyear\n",
        "df['dow'] = df['Date'].dt.dayofweek\n",
        "df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
        "df['sin_doy'] = np.sin(2*np.pi*df['dayofyear']/365.25)\n",
        "df['cos_doy'] = np.cos(2*np.pi*df['dayofyear']/365.25)"
      ],
      "metadata": {
        "id": "t5nNAoIM5DiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4) Funciones de lags y rolling (sin fuga) ---\n",
        "def add_lags(df, group_key, date_col, vars_to_lag, lags):\n",
        "    df = df.sort_values([group_key, date_col]).copy()\n",
        "    for var in vars_to_lag:\n",
        "        for k in lags:\n",
        "            df[f'{var}_lag{k}'] = df.groupby(group_key, sort=False)[var].shift(k)\n",
        "    return df\n",
        "\n",
        "def add_rolling_features(df, group_key, date_col, var, windows, stats=('mean',), shift_one=True):\n",
        "    df = df.sort_values([group_key, date_col]).copy()\n",
        "    base = df.groupby(group_key, sort=False)[var]\n",
        "    series = base.shift(1) if shift_one else base.transform(lambda x: x)\n",
        "    for w in windows:\n",
        "        roll = series.rolling(w)\n",
        "        if 'mean' in stats:\n",
        "            df[f'{var}_rollmean{w}'] = roll.mean().reset_index(level=0, drop=True)\n",
        "        if 'std' in stats:\n",
        "            df[f'{var}_rollstd{w}'] = roll.std().reset_index(level=0, drop=True)\n",
        "        if 'min' in stats:\n",
        "            df[f'{var}_rollmin{w}'] = roll.min().reset_index(level=0, drop=True)\n",
        "        if 'max' in stats:\n",
        "            df[f'{var}_rollmax{w}'] = roll.max().reset_index(level=0, drop=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "wuvaMEge5IqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5) Lags y rolling de PM25 (clave para 2026) ---\n",
        "lags_pm25 = [1, 3, 7]\n",
        "wins_pm25 = [3, 7]\n",
        "\n",
        "df_feat = add_lags(df, group_key='Estacion', date_col='Date',\n",
        "                   vars_to_lag=['PM25'], lags=lags_pm25)\n",
        "\n",
        "df_feat = add_rolling_features(df_feat, group_key='Estacion', date_col='Date',\n",
        "                               var='PM25', windows=wins_pm25,\n",
        "                               stats=('mean',), shift_one=True)\n"
      ],
      "metadata": {
        "id": "_69fnKxR5aEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6) (Opcional) Lags de meteorología (sí disponibles si tienes meteo 2026 o usarás climatología)\n",
        "include_meteo_lags = True\n",
        "meteo_vars = ['Temp','Hum','WindSpeed','Precip','Pres']\n",
        "if include_meteo_lags:\n",
        "    df_feat = add_lags(df_feat, group_key='Estacion', date_col='Date',\n",
        "                       vars_to_lag=meteo_vars, lags=[1, 3])\n"
      ],
      "metadata": {
        "id": "2Qojc8eY7V7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8) Limpieza por NaN de bordes (debidos a lags/rolling) ---\n",
        "rows_before = len(df_feat)\n",
        "df_model = df_feat.dropna(subset=['PM25_lag1','PM25_rollmean3']).reset_index(drop=True)\n",
        "rows_after = len(df_model)\n",
        "print(f\"Filas antes: {rows_before:,} | después de dropna: {rows_after:,} | perdidas: {rows_before - rows_after:,}\")\n"
      ],
      "metadata": {
        "id": "zYv2EzGI7Yoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9) Columnas finales para modelado (no entrenamos aún) ---\n",
        "base_cols = ['Date','Estacion','Localidad','lat','lon','Altitud','PM25',\n",
        "             'year','month','dayofyear','dow','is_weekend','sin_doy','cos_doy']\n",
        "lag_cols = [c for c in df_model.columns if c.startswith('PM25_lag') or c.startswith('PM25_rollmean')]\n",
        "met_lag_cols = [c for c in df_model.columns if any(c.startswith(v+'_lag') for v in meteo_vars)]\n",
        "\n",
        "cols_for_next_steps = base_cols + lag_cols + met_lag_cols\n",
        "df_ready = df_model[cols_for_next_steps].copy()\n",
        "\n",
        "print(\"Columnas finales (primeras 25):\")\n",
        "print(df_ready.columns.tolist()[:25], '...')\n",
        "df_ready.head()\n"
      ],
      "metadata": {
        "id": "7akiDNIY7cAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10) Guardar dataset de características ---\n",
        "df_ready.to_csv('df_features_PM25_no_copollutants.csv', index=False)\n",
        "print(\"✅ Guardado: df_features_PM25_no_copollutants.csv\")\n"
      ],
      "metadata": {
        "id": "TeKGc0wL7pNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Climatología meteo por estación y día-del-año (mediana) ---\n",
        "years_hist = [2021, 2022, 2023, 2024]   # ajusta si procede\n",
        "meteo_vars = ['Temp','Hum','WindSpeed','Precip','Pres']\n",
        "\n",
        "df_hist = df[df['year'].isin(years_hist)].copy()\n",
        "df_hist['doy'] = df_hist['Date'].dt.dayofyear\n",
        "\n",
        "clima = (df_hist.groupby(['Estacion','doy'])[meteo_vars]\n",
        "         .median()\n",
        "         .reset_index()\n",
        "         .rename(columns={v: f'{v}_clim' for v in meteo_vars}))\n",
        "\n",
        "# Construir calendario 2026 y “pegar” climatología por estación y DOY\n",
        "cal2026 = pd.date_range('2026-01-01','2026-12-31',freq='D')\n",
        "cal = (pd.DataFrame({'Date': cal2026})\n",
        "       .assign(doy=lambda x: x['Date'].dt.dayofyear)\n",
        "      )\n",
        "\n",
        "# Ejemplo: climatología para todas las estaciones (repetimos por estación)\n",
        "ests = df['Estacion'].dropna().unique()\n",
        "clima2026 = (cal.assign(key=1)\n",
        "               .merge(pd.DataFrame({'Estacion': ests, 'key':1}), on='key')\n",
        "               .drop(columns='key')\n",
        "               .merge(clima, on=['Estacion','doy'], how='left'))\n",
        "\n",
        "clima2026.to_csv('climatologia_meteo_2026_por_estacion.csv', index=False)\n",
        "print(\"✅ Guardado: climatologia_meteo_2026_por_estacion.csv (medianas por DOY y estación)\")\n"
      ],
      "metadata": {
        "id": "iPQnTvzu79Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Validación Cruzada**"
      ],
      "metadata": {
        "id": "HmPQS_VUCrvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "df = df_ready.copy()\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Chequeo rápido:\n",
        "print(df.shape)\n",
        "print(df[['Date','Estacion','Localidad']].head(3))\n"
      ],
      "metadata": {
        "id": "_peFGi4qCukg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columnas base (ajusta si cambiaste nombres)\n",
        "base_cols = ['Date','Localidad','lat','lon','Altitud','PM25',\n",
        "             'year','month','dayofyear','dow','is_weekend','sin_doy','cos_doy']\n",
        "\n",
        "# Lags/rollings ya construidos en Paso 1\n",
        "lag_cols = [c for c in df.columns if c.startswith('PM25_lag') or c.startswith('PM25_rollmean')]\n",
        "# Lags meteo si los añadiste en Paso 1\n",
        "met_vars = ['Temp','Hum','WindSpeed','Precip','Pres']\n",
        "met_lag_cols = [c for c in df.columns if any(c.startswith(v+'_lag') for v in met_vars)]\n",
        "\n",
        "# Sin copolutantes:\n",
        "feature_cols = ['Localidad','lat','lon','Altitud',\n",
        "                'year','month','dayofyear','dow','is_weekend','sin_doy','cos_doy'] \\\n",
        "               + lag_cols + met_lag_cols\n",
        "\n",
        "# Quitar filas con NaN en features/target (bordes por lags)\n",
        "data = df.dropna(subset=feature_cols + ['PM25']).copy()\n",
        "\n",
        "X = data[feature_cols].copy()\n",
        "y = data['PM25'].values\n",
        "dates = data['Date'].copy()\n",
        "localities = data['Localidad'].copy()\n",
        "\n",
        "# Preprocesamiento: OHE para categóricas; numéricas 'passthrough'\n",
        "cat_features = ['Localidad']\n",
        "num_features = [c for c in feature_cols if c not in cat_features]\n",
        "\n",
        "pre = ColumnTransformer([\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features),\n",
        "    ('num', 'passthrough', num_features)\n",
        "])\n"
      ],
      "metadata": {
        "id": "vGTbGcQNDXL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HistGradientBoostingRegressor(\n",
        "    learning_rate=0.06,\n",
        "    max_iter=400,\n",
        "    min_samples_leaf=25,\n",
        "    early_stopping=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('pre', pre),\n",
        "    ('model', model)\n",
        "])\n"
      ],
      "metadata": {
        "id": "nNNUt5cXDsGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_time_folds(unique_dates, n_folds=4):\n",
        "    \"\"\"\n",
        "    Forward-chaining con validación por bloques de igual tamaño aproximado.\n",
        "    Devuelve lista de dicts con índices booleanos para train/val.\n",
        "    \"\"\"\n",
        "    unique_dates = np.array(sorted(unique_dates))\n",
        "    folds = []\n",
        "    val_block = int(len(unique_dates)/(n_folds+1))\n",
        "    for k in range(1, n_folds+1):\n",
        "        train_end = k*val_block\n",
        "        val_start = train_end\n",
        "        val_end = val_start + val_block\n",
        "        train_dates = unique_dates[:train_end]\n",
        "        val_dates = unique_dates[val_start:val_end]\n",
        "        folds.append({\n",
        "            \"train_dates\": train_dates,\n",
        "            \"val_dates\": val_dates\n",
        "        })\n",
        "    return folds\n",
        "\n",
        "unique_days = np.array(sorted(dates.dt.normalize().unique()))\n",
        "folds = build_time_folds(unique_days, n_folds=4)\n",
        "[(f[\"train_dates\"].min(), f[\"train_dates\"].max(), f[\"val_dates\"].min(), f[\"val_dates\"].max()) for f in folds]\n"
      ],
      "metadata": {
        "id": "jOn_EtljDw6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import sklearn\n",
        "print(\"scikit-learn versión:\", sklearn.__version__)\n",
        "\n",
        "def rmse_compat(y_true, y_pred):\n",
        "    \"\"\"RMSE compatible con cualquier versión de scikit-learn.\"\"\"\n",
        "    try:\n",
        "        # sklearn >= 0.22 (aprox.) soporta 'squared'\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        # fallback para versiones antiguas\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "cKyRiSe5EOkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def rmse_compat(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Cambiar agrupamiento de 'Estacion' a 'Localidad'\n",
        "def eval_by_locality(y_true, y_pred, localities):\n",
        "    dfm = pd.DataFrame({\"y\": y_true, \"yhat\": y_pred, \"Localidad\": localities})  # Cambio aquí\n",
        "    out = []\n",
        "    for loc, g in dfm.groupby(\"Localidad\", sort=False):  # Cambiar 'Estacion' por 'Localidad'\n",
        "        mae = mean_absolute_error(g[\"y\"], g[\"yhat\"])\n",
        "        rmse = rmse_compat(g[\"y\"], g[\"yhat\"])   # Calculamos RMSE\n",
        "        out.append({\"Localidad\": loc, \"MAE\": mae, \"RMSE\": rmse, \"n\": len(g)})  # Cambiar 'Estacion' por 'Localidad'\n",
        "    return pd.DataFrame(out).sort_values(\"RMSE\")\n",
        "\n",
        "# Cambiar 'stations' a 'localities'\n",
        "cv_rows = []\n",
        "per_locality_reports = []\n",
        "\n",
        "for i, f in enumerate(folds, start=1):\n",
        "    tr_mask = dates.dt.normalize().isin(f[\"train_dates\"])\n",
        "    va_mask = dates.dt.normalize().isin(f[\"val_dates\"])\n",
        "\n",
        "    X_tr, y_tr = X[tr_mask], y[tr_mask]\n",
        "    X_va, y_va = X[va_mask], y[va_mask]\n",
        "    locality_va = localities[va_mask].values  # Usamos 'localities' aquí en lugar de 'stations'\n",
        "\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    pred_va = pipe.predict(X_va)\n",
        "\n",
        "    mae = mean_absolute_error(y_va, pred_va)\n",
        "    rmse = rmse_compat(y_va, pred_va)  # Calculamos RMSE\n",
        "\n",
        "    cv_rows.append({\n",
        "        \"fold\": i,\n",
        "        \"train_start\": str(dates[tr_mask].min().date()),\n",
        "        \"train_end\":   str(dates[tr_mask].max().date()),\n",
        "        \"val_start\":   str(dates[va_mask].min().date()),\n",
        "        \"val_end\":     str(dates[va_mask].max().date()),\n",
        "        \"n_train\": int(tr_mask.sum()),\n",
        "        \"n_val\": int(va_mask.sum()),\n",
        "        \"MAE\": mae,\n",
        "        \"RMSE\": rmse\n",
        "    })\n",
        "\n",
        "    # Usamos la nueva función con 'Localidad'\n",
        "    rep = eval_by_locality(y_va, pred_va, locality_va)\n",
        "    rep.insert(0, 'fold', i)\n",
        "    per_locality_reports.append(rep)\n",
        "\n",
        "cv_table = pd.DataFrame(cv_rows)\n",
        "per_locality_table = pd.concat(per_locality_reports, ignore_index=True)\n",
        "\n",
        "display(cv_table.round(3))\n",
        "display(per_locality_table.round(3))\n",
        "\n",
        "# Guardamos los resultados\n",
        "cv_table.to_csv(\"cv_temporal_global.csv\", index=False)\n",
        "per_locality_table.to_csv(\"cv_temporal_por_localidad.csv\", index=False)\n",
        "print(\"✅ Guardados: cv_temporal_global.csv, cv_temporal_por_localidad.csv\")\n"
      ],
      "metadata": {
        "id": "Tvpf1Z27D43t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_train = dates.dt.year <= 2023\n",
        "mask_test  = dates.dt.year == 2024\n",
        "\n",
        "X_tr, y_tr = X[mask_train], y[mask_train]\n",
        "X_te, y_te = X[mask_test],  y[mask_test]\n",
        "localities_te = localities[mask_test].values  # Cambiar 'stations' por 'localities'\n",
        "\n",
        "pipe.fit(X_tr, y_tr)\n",
        "pred_te = pipe.predict(X_te)\n",
        "\n",
        "mae_te = mean_absolute_error(y_te, pred_te)\n",
        "rmse_te = rmse_compat(y_te, pred_te)  # Calculando RMSE sin squared=False\n",
        "print(f\"Hold-out 2024 → MAE: {mae_te:.3f} | RMSE: {rmse_te:.3f} | n_test: {mask_test.sum()}\")\n",
        "\n",
        "# Evaluar por localidad en vez de estación\n",
        "rep_te = eval_by_locality(y_te, pred_te, localities_te).round(3)  # Usar eval_by_locality\n",
        "display(rep_te)\n",
        "\n",
        "# Guardar el resultado con nombre modificado\n",
        "rep_te.to_csv(\"holdout2024_por_localidad.csv\", index=False)\n",
        "print(\"✅ Guardado: holdout2024_por_localidad.csv\")"
      ],
      "metadata": {
        "id": "GRVe4bM3EwRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repite el armado del fold 4 exactamente como en tu CV\n",
        "fold = folds[3]  # 4º fold (índice 3)\n",
        "tr_mask = dates.dt.normalize().isin(fold[\"train_dates\"])\n",
        "va_mask = dates.dt.normalize().isin(fold[\"val_dates\"])\n",
        "\n",
        "pipe.fit(X[tr_mask], y[tr_mask])\n",
        "pred_va = pipe.predict(X[va_mask])\n",
        "\n",
        "df_va = data.loc[va_mask, ['Date','Localidad','PM25']].copy()  # Cambiar 'Estacion' por 'Localidad'\n",
        "df_va['yhat'] = pred_va\n",
        "df_va['abs_err'] = (df_va['PM25'] - df_va['yhat']).abs()\n",
        "\n",
        "# Modificar la función para agrupar por 'Localidad' en vez de 'Estacion'\n",
        "def mae_rmse(g):\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "    import numpy as np\n",
        "    try:\n",
        "        rmse = mean_squared_error(g['PM25'], g['yhat'], squared=False)\n",
        "    except TypeError:\n",
        "        rmse = np.sqrt(mean_squared_error(g['PM25'], g['yhat']))\n",
        "    return pd.Series({\n",
        "        'n': len(g),\n",
        "        'MAE': mean_absolute_error(g['PM25'], g['yhat']),\n",
        "        'RMSE': rmse\n",
        "    })\n",
        "\n",
        "# Agrupar por 'Localidad' en vez de 'Estacion'\n",
        "print(df_va.groupby('Localidad').apply(mae_rmse).round(3))\n",
        "\n",
        "# Si quieres ver la distribución de errores de Usaquen (ahora por localidad)\n",
        "usa = df_va[df_va['Localidad'] == 'Puente Aranda']  # Cambiar 'Estacion' por 'Localidad'\n",
        "print(usa[['abs_err']].describe(percentiles=[.5,.9,.95,.99]).round(3).T)\n",
        "print(\"Fechas con mayor error en Puente Aranda:\\n\", usa.nlargest(5, 'abs_err')[['Date', 'PM25', 'yhat', 'abs_err']])\n"
      ],
      "metadata": {
        "id": "7xyJypz-Jr_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CatBoost**"
      ],
      "metadata": {
        "id": "Bk-osSdQDXDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 0) Setup: instalar CatBoost (si hiciera falta) ====\n",
        "!pip -q install catboost\n"
      ],
      "metadata": {
        "id": "yCqzIId8Mv3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 1) Cargar librerías y datos ====\n",
        "import json, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# Compatibilidad de métricas (RMSE con y sin 'squared')\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "def rmse_compat(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "df = df_ready.copy()\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Solo por seguridad: tipar categóricas como string\n",
        "for c in ['Localidad']:  # Cambiar 'Estacion' por 'Localidad'\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].astype(str)\n",
        "\n",
        "print(df.shape)\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "f_D5N8ENM2Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 2) Definir features y target (sin copolutantes) ====\n",
        "base_cols = ['Date','Localidad','lat','lon','Altitud',  # Cambiar 'Estacion' por 'Localidad'\n",
        "             'year','month','dayofyear','dow','is_weekend','sin_doy','cos_doy']\n",
        "\n",
        "lag_cols = [c for c in df.columns if c.startswith('PM25_lag') or c.startswith('PM25_rollmean')]\n",
        "\n",
        "met_vars = ['Temp','Hum','WindSpeed','Precip','Pres']\n",
        "met_lag_cols = [c for c in df.columns if any(c.startswith(v+'_lag') for v in met_vars)]\n",
        "\n",
        "# target\n",
        "target_col = 'PM25'\n",
        "\n",
        "# columnas finales de X (Date no va al modelo)\n",
        "feature_cols = ['Localidad','lat','lon','Altitud',  # Cambiar 'Estacion' por 'Localidad'\n",
        "                'year','month','dayofyear','dow','is_weekend','sin_doy','cos_doy'] \\\n",
        "               + lag_cols + met_lag_cols\n",
        "\n",
        "data = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "X = data[feature_cols].copy()\n",
        "y = data[target_col].values\n",
        "dates = data['Date'].copy()\n",
        "localities = data['Localidad'].copy()  # Cambiar 'Estacion' por 'Localidad'\n",
        "\n",
        "# Índices de categóricas para CatBoost (dentro de X)\n",
        "cat_cols = ['Localidad']  # Cambiar 'Estacion' por 'Localidad'\n",
        "cat_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n",
        "\n",
        "len(feature_cols), feature_cols[:8], cat_idx\n"
      ],
      "metadata": {
        "id": "7i84Tgl_j9kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 3) Construir los mismos folds temporales (ventana expansiva) ====\n",
        "def build_time_folds(unique_dates, n_folds=4):\n",
        "    unique_dates = np.array(sorted(unique_dates))\n",
        "    folds = []\n",
        "    val_block = int(len(unique_dates)/(n_folds+1))\n",
        "    for k in range(1, n_folds+1):\n",
        "        train_end = k*val_block\n",
        "        val_start = train_end\n",
        "        val_end = val_start + val_block\n",
        "        train_dates = unique_dates[:train_end]\n",
        "        val_dates = unique_dates[val_start:val_end]\n",
        "        folds.append({\"train_dates\": train_dates, \"val_dates\": val_dates})\n",
        "    return folds\n",
        "\n",
        "unique_days = np.array(sorted(dates.dt.normalize().unique()))\n",
        "folds = build_time_folds(unique_days, n_folds=4)\n",
        "\n",
        "[(f[\"train_dates\"].min(), f[\"train_dates\"].max(), f[\"val_dates\"].min(), f[\"val_dates\"].max()) for f in folds]\n"
      ],
      "metadata": {
        "id": "wbK-YYlfNN6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 4) Función de evaluación para un set de hiperparámetros ====\n",
        "def eval_params(params, verbose=False):\n",
        "    \"\"\"Devuelve dict con MAE y RMSE promediados en CV, y por fold.\"\"\"\n",
        "    fold_results = []\n",
        "    for i, f in enumerate(folds, start=1):\n",
        "        tr_mask = dates.dt.normalize().isin(f[\"train_dates\"])\n",
        "        va_mask = dates.dt.normalize().isin(f[\"val_dates\"])\n",
        "\n",
        "        X_tr, y_tr = X[tr_mask], y[tr_mask]\n",
        "        X_va, y_va = X[va_mask], y[va_mask]\n",
        "\n",
        "        train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
        "        valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "            loss_function='RMSE',\n",
        "            iterations=params.get('iterations', 2000),\n",
        "            depth=params.get('depth', 7),\n",
        "            learning_rate=params.get('learning_rate', 0.06),\n",
        "            l2_leaf_reg=params.get('l2_leaf_reg', 3.0),\n",
        "            bootstrap_type=params.get('bootstrap_type', 'Bayesian'),\n",
        "            bagging_temperature=params.get('bagging_temperature', 1.0),\n",
        "            random_strength=params.get('random_strength', 0.0),\n",
        "            early_stopping_rounds=params.get('early_stopping_rounds', 100),\n",
        "            random_seed=42,\n",
        "            verbose=False,\n",
        "            allow_writing_files=False\n",
        "        )\n",
        "\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=verbose)\n",
        "        pred_va = model.predict(valid_pool)\n",
        "\n",
        "        mae = mean_absolute_error(y_va, pred_va)\n",
        "        rmse = rmse_compat(y_va, pred_va)\n",
        "\n",
        "        fold_results.append({\"fold\": i, \"MAE\": mae, \"RMSE\": rmse, \"n_val\": int(va_mask.sum())})\n",
        "\n",
        "    mae_mean = float(np.mean([r[\"MAE\"] for r in fold_results]))\n",
        "    rmse_mean = float(np.mean([r[\"RMSE\"] for r in fold_results]))\n",
        "    return {\"mae_mean\": mae_mean, \"rmse_mean\": rmse_mean, \"folds\": fold_results}\n"
      ],
      "metadata": {
        "id": "j3NeYaEeNR6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 5) Random Search liviano de hiperparámetros ====\n",
        "random.seed(42)\n",
        "search_space = {\n",
        "    \"depth\":       [5,6,7,8,9,10],\n",
        "    \"learning_rate\": [0.02, 0.03, 0.04, 0.06, 0.08, 0.10],\n",
        "    \"l2_leaf_reg\":  [1.0, 2.0, 3.0, 5.0, 7.0, 10.0],\n",
        "    \"bagging_temperature\": [0.0, 0.5, 1.0, 2.0, 3.0, 5.0],\n",
        "    \"random_strength\": [0.0, 0.1, 0.2, 0.5],\n",
        "    \"iterations\":  [1500, 2000, 2500],\n",
        "    \"bootstrap_type\": ['Bayesian'],\n",
        "    \"early_stopping_rounds\": [100]\n",
        "}\n",
        "\n",
        "def sample_params(space):\n",
        "    return {\n",
        "        \"depth\": random.choice(space[\"depth\"]),\n",
        "        \"learning_rate\": random.choice(space[\"learning_rate\"]),\n",
        "        \"l2_leaf_reg\": random.choice(space[\"l2_leaf_reg\"]),\n",
        "        \"bagging_temperature\": random.choice(space[\"bagging_temperature\"]),\n",
        "        \"random_strength\": random.choice(space[\"random_strength\"]),\n",
        "        \"iterations\": random.choice(space[\"iterations\"]),\n",
        "        \"bootstrap_type\": 'Bayesian',\n",
        "        \"early_stopping_rounds\": 100\n",
        "    }\n",
        "\n",
        "results = []\n",
        "N_TRIALS = 50  # puedes subirlo si tienes tiempo de cómputo\n",
        "for t in range(1, N_TRIALS+1):\n",
        "    params = sample_params(search_space)\n",
        "    res = eval_params(params, verbose=False)\n",
        "    res[\"params\"] = params\n",
        "    results.append(res)\n",
        "    print(f\"Trial {t}/{N_TRIALS} → RMSE_CV={res['rmse_mean']:.3f} | MAE_CV={res['mae_mean']:.3f} | {params}\")\n",
        "\n",
        "# Ordenar por RMSE (menor es mejor)\n",
        "results_sorted = sorted(results, key=lambda r: r[\"rmse_mean\"])\n",
        "best = results_sorted[0]\n",
        "print(\"\\n=== MEJOR CONFIGURACIÓN (CV) ===\")\n",
        "print(json.dumps(best[\"params\"], indent=2))\n",
        "print(\"CV → RMSE promedio:\", round(best[\"rmse_mean\"],3), \" | MAE promedio:\", round(best[\"mae_mean\"],3))\n",
        "pd.DataFrame(best[\"folds\"]).round(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv3kSlARNTtn",
        "outputId": "9864bc41-d4c0-4616-b0b6-699e09039161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1/50 → RMSE_CV=5.706 | MAE_CV=4.386 | {'depth': 10, 'learning_rate': 0.02, 'l2_leaf_reg': 1.0, 'bagging_temperature': 5.0, 'random_strength': 0.2, 'iterations': 1500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 2/50 → RMSE_CV=5.473 | MAE_CV=4.198 | {'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 10.0, 'bagging_temperature': 0.0, 'random_strength': 0.0, 'iterations': 2500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 3/50 → RMSE_CV=5.484 | MAE_CV=4.210 | {'depth': 8, 'learning_rate': 0.02, 'l2_leaf_reg': 1.0, 'bagging_temperature': 0.0, 'random_strength': 0.1, 'iterations': 1500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 4/50 → RMSE_CV=5.586 | MAE_CV=4.290 | {'depth': 9, 'learning_rate': 0.08, 'l2_leaf_reg': 1.0, 'bagging_temperature': 3.0, 'random_strength': 0.1, 'iterations': 2500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 5/50 → RMSE_CV=5.668 | MAE_CV=4.364 | {'depth': 10, 'learning_rate': 0.1, 'l2_leaf_reg': 7.0, 'bagging_temperature': 2.0, 'random_strength': 0.1, 'iterations': 2000, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 6/50 → RMSE_CV=5.540 | MAE_CV=4.259 | {'depth': 9, 'learning_rate': 0.04, 'l2_leaf_reg': 1.0, 'bagging_temperature': 0.5, 'random_strength': 0.5, 'iterations': 2000, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 7/50 → RMSE_CV=5.435 | MAE_CV=4.169 | {'depth': 7, 'learning_rate': 0.03, 'l2_leaf_reg': 2.0, 'bagging_temperature': 1.0, 'random_strength': 0.0, 'iterations': 1500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 8/50 → RMSE_CV=5.481 | MAE_CV=4.205 | {'depth': 8, 'learning_rate': 0.02, 'l2_leaf_reg': 3.0, 'bagging_temperature': 1.0, 'random_strength': 0.2, 'iterations': 1500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 9/50 → RMSE_CV=5.616 | MAE_CV=4.301 | {'depth': 10, 'learning_rate': 0.06, 'l2_leaf_reg': 7.0, 'bagging_temperature': 0.0, 'random_strength': 0.5, 'iterations': 1500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 10/50 → RMSE_CV=5.604 | MAE_CV=4.291 | {'depth': 9, 'learning_rate': 0.04, 'l2_leaf_reg': 10.0, 'bagging_temperature': 3.0, 'random_strength': 0.2, 'iterations': 2500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 11/50 → RMSE_CV=5.467 | MAE_CV=4.207 | {'depth': 6, 'learning_rate': 0.1, 'l2_leaf_reg': 1.0, 'bagging_temperature': 0.0, 'random_strength': 0.1, 'iterations': 2000, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 12/50 → RMSE_CV=5.420 | MAE_CV=4.163 | {'depth': 5, 'learning_rate': 0.03, 'l2_leaf_reg': 1.0, 'bagging_temperature': 2.0, 'random_strength': 0.2, 'iterations': 2000, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n",
            "Trial 13/50 → RMSE_CV=5.603 | MAE_CV=4.304 | {'depth': 10, 'learning_rate': 0.04, 'l2_leaf_reg': 2.0, 'bagging_temperature': 1.0, 'random_strength': 0.2, 'iterations': 1500, 'bootstrap_type': 'Bayesian', 'early_stopping_rounds': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 6) Reentrenar con mejor set (2021–2023) y evaluar en 2024 (hold-out) ====\n",
        "mask_train = (dates.dt.year <= 2023)\n",
        "mask_test  = (dates.dt.year == 2024)\n",
        "\n",
        "X_tr, y_tr = X[mask_train], y[mask_train]\n",
        "X_te, y_te = X[mask_test],  y[mask_test]\n",
        "\n",
        "train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
        "test_pool  = Pool(X_te, y_te, cat_features=cat_idx)\n",
        "\n",
        "best_params = best[\"params\"].copy()\n",
        "final_model = CatBoostRegressor(\n",
        "    loss_function='RMSE',\n",
        "    iterations=best_params[\"iterations\"],\n",
        "    depth=best_params[\"depth\"],\n",
        "    learning_rate=best_params[\"learning_rate\"],\n",
        "    l2_leaf_reg=best_params[\"l2_leaf_reg\"],\n",
        "    bootstrap_type='Bayesian',\n",
        "    bagging_temperature=best_params[\"bagging_temperature\"],\n",
        "    random_strength=best_params[\"random_strength\"],\n",
        "    early_stopping_rounds=100,\n",
        "    random_seed=42,\n",
        "    verbose=False,\n",
        "    allow_writing_files=False\n",
        ")\n",
        "\n",
        "# Usamos un pequeño conjunto de validación (último mes de 2023) para early stopping del final_model\n",
        "cutoff = pd.Timestamp('2023-12-01')\n",
        "tr_in  = dates[mask_train] < cutoff\n",
        "tr_val = (dates[mask_train] >= cutoff)\n",
        "\n",
        "final_model.fit(\n",
        "    Pool(X_tr[tr_in],  y_tr[tr_in],  cat_features=cat_idx),\n",
        "    eval_set=Pool(X_tr[tr_val], y_tr[tr_val], cat_features=cat_idx),\n",
        "    use_best_model=True, verbose=False\n",
        ")\n",
        "\n",
        "pred_te = final_model.predict(test_pool)\n",
        "mae_te = mean_absolute_error(y_te, pred_te)\n",
        "rmse_te = rmse_compat(y_te, pred_te)\n",
        "print(f\"Hold-out 2024 → MAE: {mae_te:.3f} | RMSE: {rmse_te:.3f} | n_test: {int(mask_test.sum())}\")\n",
        "\n",
        "# Guardar predicciones 2024\n",
        "out_te = data.loc[mask_test, ['Date','Localidad',target_col]].copy()  # Cambiar 'Estacion' por 'Localidad'\n",
        "out_te['yhat'] = pred_te\n",
        "out_te.to_csv('predicciones_holdout2024_catboost.csv', index=False)\n",
        "\n",
        "# Métricas por localidad (2024) - Cambiar 'Estacion' por 'Localidad'\n",
        "by_locality = out_te.groupby('Localidad').apply(  # Cambiar 'Estacion' por 'Localidad'\n",
        "    lambda g: pd.Series({\n",
        "        'n': len(g),\n",
        "        'MAE': mean_absolute_error(g[target_col], g['yhat']),\n",
        "        'RMSE': rmse_compat(g[target_col], g['yhat'])\n",
        "    })\n",
        ").reset_index().sort_values('RMSE')\n",
        "by_locality.round(3)\n"
      ],
      "metadata": {
        "id": "8QjYZ8Bekmpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 7) Importancias de variables y guardado del modelo ====\n",
        "# Importancia \"Feature Importance\" de CatBoost (Gain)\n",
        "fi = final_model.get_feature_importance(train_pool, type='FeatureImportance')\n",
        "fi_df = pd.DataFrame({'feature': X.columns, 'importance': fi}).sort_values('importance', ascending=False)\n",
        "fi_df.to_csv('feature_importance_catboost.csv', index=False)\n",
        "\n",
        "# Guardar modelo y parámetros\n",
        "final_model.save_model('catboost_pm25_model.cbm')\n",
        "with open('best_params_catboost.json','w') as f:\n",
        "    json.dump(best_params, f, indent=2)\n",
        "\n",
        "print(\"✅ Guardados: catboost_pm25_model.cbm, best_params_catboost.json, feature_importance_catboost.csv, predicciones_holdout2024_catboost.csv\")\n",
        "fi_df.head(15)\n"
      ],
      "metadata": {
        "id": "uKPOmtrhNaR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- 1) Evaluación de varianza explicada (R²) para cada fold y hold-out 2024 ---\n",
        "def evaluate_r2(y_true, y_pred):\n",
        "    \"\"\"Calcula R² (varianza explicada) global.\"\"\"\n",
        "    return r2_score(y_true, y_pred)\n",
        "\n",
        "# --- 2) Evaluación en los folds de la CV temporal ---\n",
        "def eval_r2_params(params, verbose=False):\n",
        "    \"\"\"Devuelve el R² promedio en CV y por fold.\"\"\"\n",
        "    fold_results = []\n",
        "    for i, f in enumerate(folds, start=1):\n",
        "        tr_mask = dates.dt.normalize().isin(f[\"train_dates\"])\n",
        "        va_mask = dates.dt.normalize().isin(f[\"val_dates\"])\n",
        "\n",
        "        X_tr, y_tr = X[tr_mask], y[tr_mask]\n",
        "        X_va, y_va = X[va_mask], y[va_mask]\n",
        "\n",
        "        train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
        "        valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "            loss_function='RMSE',\n",
        "            iterations=params.get('iterations', 2000),\n",
        "            depth=params.get('depth', 7),\n",
        "            learning_rate=params.get('learning_rate', 0.06),\n",
        "            l2_leaf_reg=params.get('l2_leaf_reg', 3.0),\n",
        "            bootstrap_type='Bayesian',\n",
        "            bagging_temperature=params.get('bagging_temperature', 1.0),\n",
        "            random_strength=params.get('random_strength', 0.0),\n",
        "            early_stopping_rounds=100,\n",
        "            random_seed=42,\n",
        "            verbose=False,\n",
        "            allow_writing_files=False\n",
        "        )\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=verbose)\n",
        "\n",
        "        # Predicciones para R² (validación)\n",
        "        yhat_va = model.predict(valid_pool)\n",
        "\n",
        "        # Calcular R² por fold\n",
        "        r2 = evaluate_r2(y_va, yhat_va)\n",
        "\n",
        "        fold_results.append({\"fold\": i, \"R²\": r2, \"n_val\": int(va_mask.sum())})\n",
        "\n",
        "    r2_mean = float(np.mean([r[\"R²\"] for r in fold_results]))\n",
        "    return {\"r2_mean\": r2_mean, \"folds\": fold_results}\n",
        "\n",
        "# Evaluación del mejor modelo (con parámetros encontrados en la búsqueda)\n",
        "r2_result = eval_r2_params(best[\"params\"])\n",
        "print(\"CV (varianza explicada R²) → R² promedio:\", round(r2_result[\"r2_mean\"],3))\n",
        "pd.DataFrame(r2_result[\"folds\"]).round(3)\n"
      ],
      "metadata": {
        "id": "Us1OeHn2dmA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3) Evaluación en hold-out 2024 (R²) ---\n",
        "mask_train = (dates.dt.year <= 2023)\n",
        "mask_test  = (dates.dt.year == 2024)\n",
        "\n",
        "X_tr, y_tr = X[mask_train], y[mask_train]\n",
        "X_te, y_te = X[mask_test],  y[mask_test]\n",
        "\n",
        "train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
        "test_pool  = Pool(X_te, y_te, cat_features=cat_idx)\n",
        "\n",
        "final_model = CatBoostRegressor(\n",
        "    loss_function='RMSE',\n",
        "    iterations=best[\"params\"][\"iterations\"],\n",
        "    depth=best[\"params\"][\"depth\"],\n",
        "    learning_rate=best[\"params\"][\"learning_rate\"],\n",
        "    l2_leaf_reg=best[\"params\"][\"l2_leaf_reg\"],\n",
        "    bootstrap_type='Bayesian',\n",
        "    bagging_temperature=best[\"params\"][\"bagging_temperature\"],\n",
        "    random_strength=best[\"params\"][\"random_strength\"],\n",
        "    early_stopping_rounds=100,\n",
        "    random_seed=42,\n",
        "    verbose=False,\n",
        "    allow_writing_files=False\n",
        ")\n",
        "\n",
        "final_model.fit(train_pool, eval_set=test_pool, use_best_model=True, verbose=False)\n",
        "\n",
        "# Predicciones en hold-out\n",
        "yhat_te = final_model.predict(test_pool)\n",
        "\n",
        "# Calcular R² en hold-out\n",
        "r2_te = evaluate_r2(y_te, yhat_te)\n",
        "print(f\"Hold-out 2024 → R²: {r2_te:.3f} | n_test: {int(mask_test.sum())}\")\n"
      ],
      "metadata": {
        "id": "OUiJ-dSodrp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4) R² por estación en hold-out 2024 ---\n",
        "def eval_r2_by_station(y_true, y_pred, localities):\n",
        "    \"\"\"Calcula R² por estación.\"\"\"\n",
        "    dfm = pd.DataFrame({\"y\": y_true, \"yhat\": y_pred, \"Localidad\": localities})\n",
        "    out = []\n",
        "    for est, g in dfm.groupby(\"Localidad\", sort=False):\n",
        "        r2 = evaluate_r2(g[\"y\"], g[\"yhat\"])\n",
        "        out.append({\"Localidad\": est, \"R²\": r2, \"n\": len(g)})\n",
        "    return pd.DataFrame(out).sort_values(\"R²\", ascending=False)\n",
        "\n",
        "r2_by_station = eval_r2_by_station(y_te, yhat_te, localities[mask_test])\n",
        "r2_by_station.round(3)\n"
      ],
      "metadata": {
        "id": "ne4G_G0wdv6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== BLOQUE A2: Rolling-origin con reentrenamiento por origen =====================\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def rmse_compat(y_true, y_pred):\n",
        "    try:    return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError: return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Reconstruye dataset coherente (igual que en el fix anterior)\n",
        "data_bt = df_ready.dropna(subset=feature_cols + ['PM25']).copy()\n",
        "X_bt     = data_bt[feature_cols].copy()\n",
        "y_bt     = data_bt['PM25'].to_numpy()\n",
        "dates_bt = pd.to_datetime(data_bt['Date'])\n",
        "cat_features = ['Localidad']\n",
        "\n",
        "# Orígenes y horizontes\n",
        "origins  = pd.date_range(\"2024-01-01\", \"2024-10-01\", freq=\"MS\")\n",
        "HORIZONS = [1, 7, 14, 30]\n",
        "\n",
        "# Hiperparámetros (usa los de tu mejor búsqueda si existen)\n",
        "if \"best\" in globals() and isinstance(best, dict) and \"params\" in best:\n",
        "    bp = best[\"params\"]\n",
        "    base_params = dict(\n",
        "        loss_function='RMSE',\n",
        "        iterations=bp.get(\"iterations\", 2000),\n",
        "        depth=bp.get(\"depth\", 8),\n",
        "        learning_rate=bp.get(\"learning_rate\", 0.06),\n",
        "        l2_leaf_reg=bp.get(\"l2_leaf_reg\", 3.0),\n",
        "        bootstrap_type='Bayesian',\n",
        "        bagging_temperature=bp.get(\"bagging_temperature\", 1.0),\n",
        "        random_strength=bp.get(\"random_strength\", 0.0),\n",
        "        early_stopping_rounds=100,\n",
        "        random_seed=42,\n",
        "        verbose=False,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "else:\n",
        "    base_params = dict(\n",
        "        loss_function='RMSE',\n",
        "        iterations=2500,\n",
        "        depth=8,\n",
        "        learning_rate=0.06,\n",
        "        l2_leaf_reg=8.0,\n",
        "        bootstrap_type='Bayesian',\n",
        "        bagging_temperature=1.0,\n",
        "        random_strength=0.0,\n",
        "        early_stopping_rounds=100,\n",
        "        random_seed=42,\n",
        "        verbose=False,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "\n",
        "rows = []\n",
        "for ori in origins:\n",
        "    # Entrenamiento con historia hasta el día anterior al origen\n",
        "    m_train = dates_bt < ori\n",
        "    if m_train.sum() < 200:\n",
        "        continue\n",
        "\n",
        "    X_tr, y_tr = X_bt.loc[m_train], y_bt[m_train.to_numpy()]\n",
        "    # Early stopping en los últimos ~60 días previos al origen\n",
        "    dt_tr = dates_bt.loc[m_train]\n",
        "    if (dt_tr.max() - dt_tr.min()).days < 120:\n",
        "        m_in  = np.ones(len(X_tr), dtype=bool)\n",
        "        m_val = np.zeros(len(X_tr), dtype=bool)\n",
        "    else:\n",
        "        cutoff = dt_tr.max() - pd.Timedelta(days=60)\n",
        "        m_in  = (dt_tr <  cutoff).to_numpy()\n",
        "        m_val = (dt_tr >= cutoff).to_numpy()\n",
        "\n",
        "    train_pool = Pool(X_tr.loc[m_in],  y_tr[m_in],  cat_features=cat_features)\n",
        "    valid_pool = Pool(X_tr.loc[m_val], y_tr[m_val], cat_features=cat_features) if m_val.any() else None\n",
        "\n",
        "    model = CatBoostRegressor(**base_params)\n",
        "    if valid_pool is not None and m_val.any():\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n",
        "    else:\n",
        "        model.fit(train_pool, verbose=False)\n",
        "\n",
        "    # Ventana a evaluar: [ori, ori+H-1] ∩ 2024\n",
        "    for H in HORIZONS:\n",
        "        endH = ori + pd.Timedelta(days=H-1)\n",
        "        m_eval = (dates_bt >= ori) & (dates_bt <= endH) & (dates_bt.dt.year == 2024)\n",
        "        if not m_eval.any():\n",
        "            continue\n",
        "\n",
        "        X_ev = X_bt.loc[m_eval]\n",
        "        y_ev = y_bt[m_eval.to_numpy()]\n",
        "        pool_ev = Pool(X_ev, label=y_ev, cat_features=cat_features)\n",
        "        yhat = model.predict(pool_ev)\n",
        "\n",
        "        mae  = mean_absolute_error(y_ev, yhat)\n",
        "        try:\n",
        "            rmse = mean_squared_error(y_ev, yhat, squared=False)\n",
        "        except TypeError:\n",
        "            rmse = np.sqrt(mean_squared_error(y_ev, yhat))\n",
        "        r2   = r2_score(y_ev, yhat)\n",
        "\n",
        "        rows.append([ori.strftime(\"%Y-%m-%d\"), H, int(m_eval.sum()), mae, rmse, r2])\n",
        "\n",
        "bt2 = pd.DataFrame(rows, columns=[\"origin\",\"horizon_days\",\"n\",\"MAE\",\"RMSE\",\"R2\"])\n",
        "print(\"Resumen por horizonte (promedio sobre orígenes):\")\n",
        "display(bt2.groupby(\"horizon_days\")[[\"MAE\",\"RMSE\",\"R2\",\"n\"]].mean().round(3))\n",
        "bt2.to_csv(\"backtest_rolling_origin_retrain_2024.csv\", index=False)\n",
        "print(\"✅ Guardado: backtest_rolling_origin_retrain_2024.csv\")\n"
      ],
      "metadata": {
        "id": "EueXMXd3zYor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== BLOQUE A2 (COMPLETO): Rolling-origin con reentrenamiento + skill + blend H=1 =====================\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def rmse_compat(y_true, y_pred):\n",
        "    try:    return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError: return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# --- 0) Reconstruir dataset coherente a partir de tu df_ready / feature_cols ---\n",
        "data_bt = df_ready.dropna(subset=feature_cols + ['PM25']).copy()\n",
        "# matrices alineadas\n",
        "X_bt     = data_bt[feature_cols].copy()\n",
        "y_bt     = data_bt['PM25'].to_numpy()\n",
        "dates_bt = pd.to_datetime(data_bt['Date']).copy()\n",
        "loc_bt   = data_bt['Localidad'].astype(str).copy()\n",
        "\n",
        "# categóricas por NOMBRE (más robusto si cambian índices)\n",
        "cat_features = ['Localidad']\n",
        "\n",
        "# índice booleano para TEST 2024 (lo piden en otros bloques)\n",
        "test_2024_idx = (dates_bt.dt.year == 2024).to_numpy()\n",
        "\n",
        "# --- 1) Orígenes y horizontes ---\n",
        "origins  = pd.date_range(\"2024-01-01\", \"2024-10-01\", freq=\"MS\")\n",
        "HORIZONS = [1, 7, 14, 30]\n",
        "\n",
        "# --- 2) Hiperparámetros base (usa los de tu búsqueda si existen) ---\n",
        "if \"best\" in globals() and isinstance(best, dict) and \"params\" in best:\n",
        "    bp = best[\"params\"]\n",
        "    base_params = dict(\n",
        "        loss_function='MAE',          # MAE mejora robustez para H=1\n",
        "        eval_metric='MAE',\n",
        "        iterations=bp.get(\"iterations\", 2000),\n",
        "        depth=bp.get(\"depth\", 8),\n",
        "        learning_rate=bp.get(\"learning_rate\", 0.06),\n",
        "        l2_leaf_reg=bp.get(\"l2_leaf_reg\", 3.0),\n",
        "        bootstrap_type='Bayesian',\n",
        "        bagging_temperature=bp.get(\"bagging_temperature\", 1.0),\n",
        "        random_strength=bp.get(\"random_strength\", 0.0),\n",
        "        early_stopping_rounds=100,\n",
        "        random_seed=42,\n",
        "        verbose=False,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "else:\n",
        "    base_params = dict(\n",
        "        loss_function='MAE',\n",
        "        eval_metric='MAE',\n",
        "        iterations=2500,\n",
        "        depth=8,\n",
        "        learning_rate=0.06,\n",
        "        l2_leaf_reg=8.0,\n",
        "        bootstrap_type='Bayesian',\n",
        "        bagging_temperature=1.0,\n",
        "        random_strength=0.0,\n",
        "        early_stopping_rounds=100,\n",
        "        random_seed=42,\n",
        "        verbose=False,\n",
        "        allow_writing_files=False\n",
        "    )\n",
        "\n",
        "# Peso del blend con persistencia para H=1 (ajústalo 0.2–0.6 si quieres)\n",
        "W_BLEND_H1 = 0.4\n",
        "\n",
        "rows = []\n",
        "for ori in origins:\n",
        "    # --- 3) Ventana de entrenamiento: últimos 365 días antes del origen ---\n",
        "    m_train_all = (dates_bt < ori)\n",
        "    if m_train_all.sum() < 120:\n",
        "        # muy poca historia; saltar este origen\n",
        "        continue\n",
        "\n",
        "    # recorta a últimos 365 días\n",
        "    last_day_train = dates_bt.loc[m_train_all].max()\n",
        "    cut_start = last_day_train - pd.Timedelta(days=365)\n",
        "    m_train = m_train_all & (dates_bt >= cut_start)\n",
        "\n",
        "    X_tr, y_tr = X_bt.loc[m_train], y_bt[m_train.to_numpy()]\n",
        "    dt_tr = dates_bt.loc[m_train]\n",
        "\n",
        "    # --- 4) Early stopping: últimos 120 días del entrenamiento como validación ---\n",
        "    if (dt_tr.max() - dt_tr.min()).days < 180:\n",
        "        m_in  = np.ones(len(X_tr), dtype=bool)\n",
        "        m_val = np.zeros(len(X_tr), dtype=bool)\n",
        "    else:\n",
        "        cutoff = dt_tr.max() - pd.Timedelta(days=120)\n",
        "        m_in  = (dt_tr <  cutoff).to_numpy()\n",
        "        m_val = (dt_tr >= cutoff).to_numpy()\n",
        "\n",
        "    train_pool = Pool(X_tr.loc[m_in],  y_tr[m_in],  cat_features=cat_features)\n",
        "    valid_pool = Pool(X_tr.loc[m_val], y_tr[m_val], cat_features=cat_features) if m_val.any() else None\n",
        "\n",
        "    model = CatBoostRegressor(**base_params)\n",
        "    if valid_pool is not None and m_val.any():\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n",
        "    else:\n",
        "        model.fit(train_pool, verbose=False)\n",
        "\n",
        "    # --- 5) Evaluación por horizontes en 2024 ---\n",
        "    for H in HORIZONS:\n",
        "        endH = ori + pd.Timedelta(days=H-1)\n",
        "        m_eval = (dates_bt >= ori) & (dates_bt <= endH) & (dates_bt.dt.year == 2024)\n",
        "        if not m_eval.any():\n",
        "            continue\n",
        "\n",
        "        X_ev = X_bt.loc[m_eval]\n",
        "        y_ev = y_bt[m_eval.to_numpy()]\n",
        "        pool_ev = Pool(X_ev, label=y_ev, cat_features=cat_features)\n",
        "        yhat = model.predict(pool_ev)\n",
        "\n",
        "        # Baseline naive lag-1 (si hay NaN, evalúo skill solo con válidos)\n",
        "        naive_arr = data_bt.loc[m_eval, 'PM25_lag1'].to_numpy() if 'PM25_lag1' in data_bt.columns else np.full_like(y_ev, np.nan)\n",
        "        valid_mask = ~np.isnan(naive_arr)\n",
        "        rmse_naive = rmse_compat(y_ev[valid_mask], naive_arr[valid_mask]) if valid_mask.any() else np.nan\n",
        "\n",
        "        # Blend con persistencia SOLO para H=1\n",
        "        if H == 1 and valid_mask.any():\n",
        "            yhat_blend = yhat.copy()\n",
        "            yhat_blend[valid_mask] = (1.0 - W_BLEND_H1) * yhat_blend[valid_mask] + W_BLEND_H1 * naive_arr[valid_mask]\n",
        "            yhat = yhat_blend\n",
        "\n",
        "        mae  = mean_absolute_error(y_ev, yhat)\n",
        "        rmse = rmse_compat(y_ev, yhat)\n",
        "        r2   = r2_score(y_ev, yhat)\n",
        "\n",
        "        skill = np.nan\n",
        "        if valid_mask.any() and rmse_naive > 0:\n",
        "            skill = 1.0 - (rmse / rmse_naive)\n",
        "\n",
        "        rows.append([ori.strftime(\"%Y-%m-%d\"), H, int(m_eval.sum()), mae, rmse, r2, rmse_naive, skill])\n",
        "\n",
        "bt2 = pd.DataFrame(rows, columns=[\"origin\",\"horizon_days\",\"n\",\"MAE\",\"RMSE\",\"R2\",\"RMSE_naive\",\"Skill_vs_naive\"])\n",
        "summary = bt2.groupby(\"horizon_days\")[[\"MAE\",\"RMSE\",\"R2\",\"RMSE_naive\",\"Skill_vs_naive\",\"n\"]].mean().round(3)\n",
        "\n",
        "print(\"Resumen por horizonte (promedio sobre orígenes):\")\n",
        "display(summary)\n",
        "bt2.to_csv(\"backtest_rolling_origin_retrain_2024.csv\", index=False)\n",
        "summary.to_csv(\"backtest_rolling_origin_retrain_2024_summary.csv\")\n",
        "print(\"✅ Guardados: backtest_rolling_origin_retrain_2024.csv, backtest_rolling_origin_retrain_2024_summary.csv\")\n"
      ],
      "metadata": {
        "id": "gu962Kgt6y3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === FUTURO 2025–2026 A PARTIR DE TU CLIMATOLOGÍA ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Sxmuu/TG-Samuel-P/main/Databases/Climat/Forecast/Forecast_2025-2026.csv\"\n",
        "\n",
        "df_clim = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "b33aWr-O9Xja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== PROYECCIÓN 2025–2026 DESDE CLIMATOLOGÍA =====================\n",
        "import pandas as pd, numpy as np\n",
        "from collections import deque\n",
        "from catboost import Pool\n",
        "\n",
        "# ---------- ENTRADAS ESPERADAS ----------\n",
        "# df_ready: tu dataset final con estructura mencionada (incluye lags de PM25 y meteo)\n",
        "# df_clim : DataFrame con columnas ['date','station','Temp','Hum','WindSpeed','Precip','Pres'] para 2025–2026\n",
        "# final_model: CatBoost entrenado (o usa el que reentrenaste en tu cuaderno)\n",
        "# feature_cols: ya definido en tu cuaderno (usas Localidad + geo + calendario + lags PM25 + lags meteo)\n",
        "# cat_idx: índices de categóricas (en tu caso suele ser ['Localidad'])\n",
        "\n",
        "# -------- 1) Normalizar climatología y añadir metadata de estación --------\n",
        "future_exo = df_clim.rename(columns={\"date\":\"Date\",\"station\":\"Estacion\"}).copy()\n",
        "future_exo[\"Date\"] = pd.to_datetime(future_exo[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Metadata por estación desde df_ready\n",
        "meta_cols = [\"Estacion\",\"Localidad\",\"lat\",\"lon\",\"Altitud\"]\n",
        "st_meta = (df_ready[meta_cols]\n",
        "           .dropna(subset=[\"Estacion\"])\n",
        "           .drop_duplicates(\"Estacion\"))\n",
        "future_exo = future_exo.merge(st_meta, on=\"Estacion\", how=\"left\")\n",
        "\n",
        "# Si hay estaciones sin Localidad/geo, advierte (las puedes excluir o completar manualmente)\n",
        "miss = future_exo[future_exo[\"Localidad\"].isna()][\"Estacion\"].unique()\n",
        "if len(miss):\n",
        "    print(\"⚠️ Estaciones sin metadata (no tendrán predicción a menos que completes Localidad/geo):\", miss)\n",
        "\n",
        "# -------- 2) Calendario para que calce con tus features --------\n",
        "future_exo[\"year\"]       = future_exo[\"Date\"].dt.year\n",
        "future_exo[\"month\"]      = future_exo[\"Date\"].dt.month\n",
        "future_exo[\"dayofyear\"]  = future_exo[\"Date\"].dt.dayofyear\n",
        "future_exo[\"dow\"]        = future_exo[\"Date\"].dt.dayofweek\n",
        "future_exo[\"is_weekend\"] = (future_exo[\"dow\"] >= 5).astype(int)\n",
        "future_exo[\"sin_doy\"]    = np.sin(2*np.pi*future_exo[\"dayofyear\"]/365.25)\n",
        "future_exo[\"cos_doy\"]    = np.cos(2*np.pi*future_exo[\"dayofyear\"]/365.25)\n",
        "\n",
        "# -------- 3) Garantizar cobertura si faltan registros en alguna estación/fecha --------\n",
        "# Construimos una malla completa fechas×estaciones (las de df_ready) y rellenamos con climatología por Localidad–DOY y ciudad–DOY si hiciera falta.\n",
        "all_stations = st_meta[\"Estacion\"].unique()\n",
        "date_min, date_max = future_exo[\"Date\"].min(), future_exo[\"Date\"].max()\n",
        "grid = pd.DataFrame({\"Date\": pd.date_range(date_min, date_max, freq=\"D\")})\n",
        "grid = grid.assign(key=1).merge(pd.DataFrame({\"Estacion\": all_stations, \"key\":1}), on=\"key\").drop(columns=\"key\")\n",
        "\n",
        "# 3) Unir el futuro (puede venir incompleto) con la malla completa\n",
        "future_exo = grid.merge(future_exo, on=[\"Date\",\"Estacion\"], how=\"left\")\n",
        "\n",
        "# 4) Añadir metadata de estación SOLO si falta alguna de estas columnas\n",
        "meta_needed = [c for c in [\"Localidad\",\"lat\",\"lon\",\"Altitud\"] if c not in future_exo.columns]\n",
        "if meta_needed:\n",
        "    future_exo = future_exo.merge(st_meta[[\"Estacion\"] + meta_needed], on=\"Estacion\", how=\"left\")\n",
        "\n",
        "future_exo[\"doy\"] = future_exo[\"Date\"].dt.dayofyear\n",
        "met_vars = [\"Temp\",\"Hum\",\"WindSpeed\",\"Precip\",\"Pres\"]\n",
        "\n",
        "# Climatología por Localidad–DOY y Ciudad–DOY usando la MISMA climatología disponible (si te falta para ciertas estaciones)\n",
        "clim_loc  = (future_exo.groupby([\"Localidad\",\"doy\"])[met_vars].median()\n",
        "                       .rename(columns={v:f\"{v}_loc\" for v in met_vars}).reset_index())\n",
        "clim_city = (future_exo.groupby([\"doy\"])[met_vars].median()\n",
        "                       .rename(columns={v:f\"{v}_city\" for v in met_vars}).reset_index())\n",
        "\n",
        "future_exo = future_exo.merge(clim_loc,  on=[\"Localidad\",\"doy\"], how=\"left\") \\\n",
        "                       .merge(clim_city, on=[\"doy\"],       how=\"left\")\n",
        "\n",
        "glob_med = future_exo[met_vars].median(numeric_only=True)\n",
        "\n",
        "for v in met_vars:\n",
        "    # Prioridad: valor provisto por df_clim > climatología localidad-DOY > ciudad-DOY > mediana global\n",
        "    future_exo[v] = (future_exo[v]\n",
        "                     .fillna(future_exo[f\"{v}_loc\"])\n",
        "                     .fillna(future_exo[f\"{v}_city\"])\n",
        "                     .fillna(glob_med[v]))\n",
        "\n",
        "# -------- 4) Construir lags de meteo (lag1, lag3) por estación --------\n",
        "future_exo = future_exo.sort_values([\"Estacion\",\"Date\"]).reset_index(drop=True)\n",
        "for v in met_vars:\n",
        "    future_exo[f\"{v}_lag1\"] = future_exo.groupby(\"Estacion\", sort=False)[v].shift(1)\n",
        "    future_exo[f\"{v}_lag3\"] = future_exo.groupby(\"Estacion\", sort=False)[v].shift(3)\n",
        "\n",
        "# Para los primeros 1–3 días de 2025 habrá NaN en los lags; rellenamos con la misma serie (backfill ligero) para no perder esos días\n",
        "# Asegura tipos y orden antes de transformar\n",
        "future_exo[\"Estacion\"] = future_exo[\"Estacion\"].astype(str)\n",
        "future_exo = future_exo.sort_values([\"Estacion\",\"Date\"]).reset_index(drop=True)\n",
        "\n",
        "for v in [\"Temp\",\"Hum\",\"WindSpeed\",\"Precip\",\"Pres\"]:\n",
        "    for k in [1, 3]:\n",
        "        col = f\"{v}_lag{k}\"\n",
        "        # Si la columna no existe aún, sáltala\n",
        "        if col not in future_exo.columns:\n",
        "            continue\n",
        "        # Usar transform en vez de apply para mantener el mismo índice\n",
        "        future_exo[col] = (\n",
        "            future_exo\n",
        "            .groupby(\"Estacion\", sort=False)[col]\n",
        "            .transform(lambda s: s.bfill().ffill())\n",
        "            .astype(\"float64\")   # homogeneiza dtype y evita conflictos\n",
        "        )\n",
        "# -------- 5) Semilla de lags PM25 (últimos 7 días observados hasta 2024-12-31) --------\n",
        "hist_buffers = {}\n",
        "for st, g in df_ready.sort_values([\"Estacion\",\"Date\"]).groupby(\"Estacion\", sort=False):\n",
        "    tail = g[\"PM25\"].tail(7).tolist()\n",
        "    if len(tail) < 7:\n",
        "        tail = [np.nan]*(7-len(tail)) + tail\n",
        "    hist_buffers[st] = deque(tail, maxlen=7)\n",
        "\n",
        "# -------- 6) Proyección recursiva 2025–2026 (usando TUS feature_cols) --------\n",
        "future_days = sorted(future_exo[\"Date\"].unique())\n",
        "pred_rows = []\n",
        "\n",
        "for d in future_days:\n",
        "    day = future_exo[future_exo[\"Date\"] == d]\n",
        "    for st, g in day.groupby(\"Estacion\"):\n",
        "        if st not in hist_buffers:   # si la estación no estaba en df_ready (poco probable), la saltamos\n",
        "            continue\n",
        "        buf = hist_buffers[st].copy()\n",
        "\n",
        "        # Lags/rollings de PM25 desde el buffer\n",
        "        pm25_lag1 = buf[-1] if len(buf)>=1 else np.nan\n",
        "        pm25_lag3 = buf[-3] if len(buf)>=3 else np.nan\n",
        "        pm25_lag7 = buf[0]  if len(buf)>=7 else np.nan\n",
        "        roll3     = np.nanmean([x for x in list(buf)[-3:] if pd.notna(x)]) if len(buf) else np.nan\n",
        "        roll7     = np.nanmean([x for x in list(buf)      if pd.notna(x)]) if len(buf) else np.nan\n",
        "\n",
        "        row = g.iloc[0].copy()\n",
        "        row[\"PM25_lag1\"] = pm25_lag1\n",
        "        row[\"PM25_lag3\"] = pm25_lag3\n",
        "        row[\"PM25_lag7\"] = pm25_lag7\n",
        "        row[\"PM25_rollmean3\"] = roll3\n",
        "        row[\"PM25_rollmean7\"] = roll7\n",
        "\n",
        "        # Construir X EXACTAMENTE con tus 'feature_cols'\n",
        "        x = row.reindex(feature_cols, fill_value=np.nan)\n",
        "\n",
        "        # Predecir\n",
        "        pool = Pool(pd.DataFrame([x.values], columns=feature_cols), cat_features=cat_idx)\n",
        "        yhat = float(final_model.predict(pool))\n",
        "\n",
        "        pred_rows.append({\"Date\": d, \"Estacion\": row[\"Estacion\"], \"Localidad\": row[\"Localidad\"], \"PM25_pred\": yhat})\n",
        "        # actualizar buffer para el siguiente día\n",
        "        hist_buffers[st].append(yhat)\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows).sort_values([\"Estacion\",\"Date\"]).reset_index(drop=True)\n",
        "\n",
        "# -------- 7) Salidas por estación y por localidad --------\n",
        "pred_loc = (pred_df.groupby([\"Date\",\"Localidad\"])[\"PM25_pred\"]\n",
        "            .median().rename(\"PM25_pred_mediana\").reset_index())\n",
        "\n",
        "pred_df.to_csv(\"proyecciones_pm25_2025_2026_por_estacion.csv\", index=False)\n",
        "pred_loc.to_csv(\"proyecciones_pm25_2025_2026_por_localidad.csv\", index=False)\n",
        "print(\"✅ Guardados: proyecciones_pm25_2025_2026_por_estacion.csv, proyecciones_pm25_2025_2026_por_localidad.csv\")\n",
        "display(pred_df.head(), pred_loc.head())\n"
      ],
      "metadata": {
        "id": "YwkNVOHpBbn1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}